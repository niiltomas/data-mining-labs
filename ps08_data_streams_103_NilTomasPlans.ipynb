{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 08: Data streams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Nil Tomas Plans</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">nil.tomas01@estudiant.upf.edu</font>\n",
    "\n",
    "Date: <font color=\"blue\">26/11/2024</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import random\n",
    "import statistics\n",
    "import secrets\n",
    "import re\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Nil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset and how to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "INPUT_FILE = \"movie_lines.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "POS_NOUN = 'NN'\n",
    "POS_VERB = 'VB'\n",
    "POS_ADJECTIVE = 'JJ'\n",
    "\n",
    "# Producer in Python that reads a file by words that are nouns\n",
    "def read_by_parts_of_speech(filename, parts_of_speech, max_words=-1, report_every=-1):\n",
    "    \n",
    "    # Open the input file\n",
    "    with gzip.open(INPUT_FILE, \"rt\", encoding='utf8') as file:\n",
    "        \n",
    "        # Initialize counter of words to stop at max_words\n",
    "        counter = 0\n",
    "    \n",
    "        # Iterate through lines in the file\n",
    "        for line in file:\n",
    "            \n",
    "            elements = line.split(\"\\t\")\n",
    "            \n",
    "            text = \"\"\n",
    "            if len(elements) >= 5:\n",
    "                text = elements[4].strip()\n",
    "                                        \n",
    "            if counter > max_words and max_words != -1:\n",
    "                break\n",
    "                \n",
    "            for sentence in nltk.sent_tokenize(text):\n",
    "                \n",
    "                tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "                for word in [part[0] for part in tagged if part[1] in parts_of_speech]:\n",
    "                \n",
    "                    counter += 1\n",
    "\n",
    "                    # Report\n",
    "                    if (report_every != -1) and (counter % report_every == 0):\n",
    "                        if max_words == -1:\n",
    "                            print(\"- Read %d words so far\" % (counter))\n",
    "                        else:\n",
    "                            print(\"- Read %d/%d words so far\" % (counter, max_words))\n",
    "\n",
    "                    # Produce the word in lowercase\n",
    "                    yield word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current noun 'ten'\n",
      "Current noun 'wonderful'\n",
      "Current noun 'ready'\n",
      "Current noun 'clear'\n",
      "Current noun 'cheap'\n",
      "Current noun 'other'\n",
      "Current noun 'good'\n",
      "Current noun 'front'\n",
      "- Read 10000/30000 words so far\n",
      "Current noun 'good'\n",
      "Current noun 'senseless'\n",
      "Current noun 'true'\n",
      "Current noun 'funny'\n",
      "Current noun 'fine'\n",
      "Current noun 'awful'\n",
      "Current noun 'good'\n",
      "Current noun 'arrive'\n",
      "Current noun 'last'\n",
      "Current noun 'own'\n",
      "Current noun 'external'\n",
      "Current noun 'nuclear'\n",
      "Current noun 'dear'\n",
      "Current noun 'interested'\n",
      "Current noun 'other'\n",
      "- Read 20000/30000 words so far\n",
      "Current noun 'interested'\n",
      "Current noun 'many'\n",
      "Current noun 'hard'\n",
      "Current noun 'poor'\n",
      "Current noun 'own'\n",
      "Current noun 'funny'\n",
      "Current noun 'penny-ante'\n",
      "Current noun 'vivid'\n",
      "Current noun 'm-60'\n",
      "Current noun 'little'\n",
      "- Read 30000/30000 words so far\n"
     ]
    }
   ],
   "source": [
    "for word in read_by_parts_of_speech(INPUT_FILE, [POS_ADJECTIVE], max_words=30000, report_every=10000):\n",
    "    # Prints 1/1000 of words\n",
    "    if random.random() < 0.001:\n",
    "        print(\"Current noun '%s'\" % (word)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine approximately the top-10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"add_reservoir\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    if(len(reservoir)<max_reservoir_size):#if size of reservoir is less than threshold we add an item\n",
    "        reservoir.append(item)\n",
    "\n",
    "    elif len(reservoir)==max_reservoir_size:#if the reservoir is full, randomly delete an element from  reservoir and appent a new one\n",
    "        reservoir.pop(random.randint(0,len(reservoir)-1))\n",
    "        reservoir.append(item)\n",
    "\n",
    "    \n",
    "    assert(len(reservoir) <= max_reservoir_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"reservoir_sampling\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(filename,pos_adj, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "    \n",
    "    words_read = 0\n",
    "    \n",
    "    for word in read_by_parts_of_speech(filename,pos_adj, max_words=max_words, report_every=report_every):\n",
    "        \n",
    "        words_read+=1\n",
    "        #probability to ignore the word\n",
    "        probability_ignore=1-reservoir_size/words_read\n",
    "        #if the prob is less than probability to ignore do not append or pop any element of reservoir\n",
    "        if random.random() >= probability_ignore:\n",
    "            add_to_reservoir(reservoir, word, reservoir_size)\n",
    " \n",
    "        \n",
    "\n",
    "    return (words_read, reservoir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Number of items seen    : 30001\n",
      "Number of items sampled : 1500\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "reservoir_size = 1500\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=30000, report_every=10000)\n",
    "\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 good\n",
      "33 last\n",
      "30 little\n",
      "27 much\n",
      "25 right\n",
      "23 sorry\n",
      "23 dead\n",
      "20 other\n",
      "19 old\n",
      "18 wrong\n",
      "18 own\n",
      "18 next\n",
      "17 sure\n",
      "17 same\n",
      "17 bad\n",
      "16 long\n",
      "16 first\n",
      "16 big\n",
      "15 happy\n",
      "13 whole\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:20]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"%d %s\" % (absolute_frequency, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to print the top items and their relative frequencies</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 20 ITEMS MOST FREQUENT AND %\n",
      "Item 1: good 4.73%\n",
      "Item 2: last 2.20%\n",
      "Item 3: little 2.00%\n",
      "Item 4: much 1.80%\n",
      "Item 5: right 1.67%\n",
      "Item 6: sorry 1.53%\n",
      "Item 7: dead 1.53%\n",
      "Item 8: other 1.33%\n",
      "Item 9: old 1.27%\n",
      "Item 10: wrong 1.20%\n",
      "Item 11: own 1.20%\n",
      "Item 12: next 1.20%\n",
      "Item 13: sure 1.13%\n",
      "Item 14: same 1.13%\n",
      "Item 15: bad 1.13%\n",
      "Item 16: long 1.07%\n",
      "Item 17: first 1.07%\n",
      "Item 18: big 1.07%\n",
      "Item 19: happy 1.00%\n",
      "Item 20: whole 0.87%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:20]\n",
    "\n",
    "i=1\n",
    "print(\"TOP 20 ITEMS MOST FREQUENT AND %\")\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    percentage=absolute_frequency/len(reservoir)\n",
    "    print(\"Item %d: %s %.2f%%\" % (i, word,percentage*100))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Increase the max limit of words so that one pass takes about 2-3 minutes to be completed. Replace this cell with your code to try different reservoir sizes. In each case, print your estimate for the relative and absolute frequency of the words in the entire dataset.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_items_freq(num_words_top,reservoir_size,dataset_size):\n",
    "    #reservoir sampling algorithm\n",
    "    (items_seen, reservoir) = reservoir_sampling(INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=30000, report_every=10000)\n",
    "\n",
    "    print(\"Number of items seen    : %d\" % items_seen)\n",
    "    print(\"Number of items sampled : %d\" % len(reservoir) )\n",
    "\n",
    "    #compute frequencies of the top words\n",
    "    freq = {}\n",
    "    for item in reservoir:\n",
    "        freq[item] = reservoir.count(item)\n",
    "\n",
    "    most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:num_words_top]\n",
    "\n",
    "    i=1\n",
    "    print(f\" \\n TOP {num_words_top}  ITEMS MOST FREQUENT AND %\")\n",
    "    for absolute_frequency, word in most_frequent_items:\n",
    "        percentage=absolute_frequency/len(reservoir)\n",
    "        print(\"Item %d: %s %.2f%%\" % (i, word,percentage*100))\n",
    "        \n",
    "        #compute estimate in the entire dataset\n",
    "        if reservoir_size!=0:\n",
    "            estimate_freq_item_dataset=absolute_frequency*dataset_size/reservoir_size\n",
    "        else:\n",
    "            estimate_freq_item_dataset=0\n",
    "        print(f\"Estimate freq for item {i} estimate_freq_item_dataset\",estimate_freq_item_dataset)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With reservoir_size:  50 \n",
      "\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Number of items seen    : 30001\n",
      "Number of items sampled : 50\n",
      " \n",
      " TOP 5  ITEMS MOST FREQUENT AND %\n",
      "Item 1: short 4.00%\n",
      "Estimate freq for item 1 estimate_freq_item_dataset 1200.0\n",
      "Item 2: quick 4.00%\n",
      "Estimate freq for item 2 estimate_freq_item_dataset 1200.0\n",
      "Item 3: next 4.00%\n",
      "Estimate freq for item 3 estimate_freq_item_dataset 1200.0\n",
      "Item 4: many 4.00%\n",
      "Estimate freq for item 4 estimate_freq_item_dataset 1200.0\n",
      "Item 5: first 4.00%\n",
      "Estimate freq for item 5 estimate_freq_item_dataset 1200.0\n",
      "\n",
      "With reservoir_size:  100 \n",
      "\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Number of items seen    : 30001\n",
      "Number of items sampled : 100\n",
      " \n",
      " TOP 5  ITEMS MOST FREQUENT AND %\n",
      "Item 1: little 4.00%\n",
      "Estimate freq for item 1 estimate_freq_item_dataset 1200.0\n",
      "Item 2: wrong 3.00%\n",
      "Estimate freq for item 2 estimate_freq_item_dataset 900.0\n",
      "Item 3: many 3.00%\n",
      "Estimate freq for item 3 estimate_freq_item_dataset 900.0\n",
      "Item 4: good 3.00%\n",
      "Estimate freq for item 4 estimate_freq_item_dataset 900.0\n",
      "Item 5: sure 2.00%\n",
      "Estimate freq for item 5 estimate_freq_item_dataset 600.0\n",
      "\n",
      "With reservoir_size:  500 \n",
      "\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Number of items seen    : 30001\n",
      "Number of items sampled : 500\n",
      " \n",
      " TOP 5  ITEMS MOST FREQUENT AND %\n",
      "Item 1: good 4.40%\n",
      "Estimate freq for item 1 estimate_freq_item_dataset 1320.0\n",
      "Item 2: little 2.40%\n",
      "Estimate freq for item 2 estimate_freq_item_dataset 720.0\n",
      "Item 3: sure 2.00%\n",
      "Estimate freq for item 3 estimate_freq_item_dataset 600.0\n",
      "Item 4: sorry 1.80%\n",
      "Estimate freq for item 4 estimate_freq_item_dataset 540.0\n",
      "Item 5: real 1.80%\n",
      "Estimate freq for item 5 estimate_freq_item_dataset 540.0\n",
      "\n",
      "With reservoir_size:  1000 \n",
      "\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Number of items seen    : 30001\n",
      "Number of items sampled : 1000\n",
      " \n",
      " TOP 5  ITEMS MOST FREQUENT AND %\n",
      "Item 1: good 3.70%\n",
      "Estimate freq for item 1 estimate_freq_item_dataset 1110.0\n",
      "Item 2: right 2.50%\n",
      "Estimate freq for item 2 estimate_freq_item_dataset 750.0\n",
      "Item 3: sorry 2.10%\n",
      "Estimate freq for item 3 estimate_freq_item_dataset 630.0\n",
      "Item 4: other 2.10%\n",
      "Estimate freq for item 4 estimate_freq_item_dataset 630.0\n",
      "Item 5: last 2.00%\n",
      "Estimate freq for item 5 estimate_freq_item_dataset 600.0\n",
      "\n",
      "With reservoir_size:  1500 \n",
      "\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Number of items seen    : 30001\n",
      "Number of items sampled : 1500\n",
      " \n",
      " TOP 5  ITEMS MOST FREQUENT AND %\n",
      "Item 1: good 4.80%\n",
      "Estimate freq for item 1 estimate_freq_item_dataset 1440.0\n",
      "Item 2: little 2.40%\n",
      "Estimate freq for item 2 estimate_freq_item_dataset 720.0\n",
      "Item 3: right 2.00%\n",
      "Estimate freq for item 3 estimate_freq_item_dataset 600.0\n",
      "Item 4: last 1.93%\n",
      "Estimate freq for item 4 estimate_freq_item_dataset 580.0\n",
      "Item 5: other 1.87%\n",
      "Estimate freq for item 5 estimate_freq_item_dataset 560.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_words=5\n",
    "reservoir_size=[50,100,500,1000,1500]\n",
    "dataset_size=3e4\n",
    "for i in range(0,len(reservoir_size)):\n",
    "    print(\"With reservoir_size: \",reservoir_size[i],\"\\n\")\n",
    "    top_items_freq(top_words,reservoir_size[i],dataset_size)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary indicating what reservoir size you would recommend to use, and your overall conclusions.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"blue\">To chose a minimum reservoir size that I would recommend to use is a size where most of the top-3 words appear into two or more consecutive runs, therefore we obtain stable results. In our case, I would choose a reservoir of size 500, because on the iteration of this size and size = 1000 we obtain the same top-2 elements because the third element varies quite often on different reservoir sizes between [100,1500]. We observe also, that the same top-2 elements are constant on bigger reservoir sizes.\n",
    "Therefore, as we want to choose the minimum size, I'd recommend using size=500 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine approximately the distinct number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def count_trailing_zeroes(number):\n",
    "    count = 0\n",
    "    while number & 1 == 0:\n",
    "        count += 1\n",
    "        number = number >> 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def random_hash_function():\n",
    "    # We use a cryptographically safe generator for the salt of our hash function\n",
    "    salt = secrets.token_bytes(32)\n",
    "    return lambda string: hash(string + str(salt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to perform the requested number of passes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 1: 2048 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 2: 4096 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 3: 16384 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 4: 32768 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 5: 4096 distinct words\n"
     ]
    }
   ],
   "source": [
    "\n",
    "number_of_passes = 5\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    hash_function=random_hash_function()\n",
    "    R=0\n",
    "    for u in read_by_parts_of_speech(INPUT_FILE, [POS_NOUN,POS_ADJECTIVE,POS_VERB], max_words=30000, report_every=10000):\n",
    "        hash_value=hash_function(u)\n",
    "        r=count_trailing_zeroes(abs(hash_value))\n",
    "        R=max(R,r)#maximum value of r(u) seen so far\n",
    "    \n",
    "    estimate=2**R#Add 2^R as an estimate for the number of distinct elements u seen\n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Average of estimates: 11878.4\n",
      "* Median  of estimates: 4096.0\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Compute the median of average estimates in 3 separate runs of your algorithm; each run should do 10 passes over the file. Repeat this for nouns (POS_NOUN), adjectives (POS_ADJECTIVE), and verbs (POS_VERB). Replace this cell with the results you obtained in each pass, and whether the average or the median seem more appropriate for this probabilistic counting.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flajolet_martin_alg_avg_med(number_of_passes, POS_):\n",
    "    estimates = []\n",
    "\n",
    "    for i in range(number_of_passes):\n",
    "        hash_function=random_hash_function()\n",
    "        R=0\n",
    "        for u in read_by_parts_of_speech(INPUT_FILE, POS_, max_words=3000, report_every=1000):\n",
    "            hash_value=hash_function(u)\n",
    "            r=count_trailing_zeroes(abs(hash_value))\n",
    "            R=max(R,r)#maximum value of r(u) seen so far\n",
    "\n",
    "        estimate=2**R#Add 2^R as an estimate for the number of distinct elements u seen\n",
    "        estimates.append(estimate)\n",
    "        print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "        \n",
    "    print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "    print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flajolet-Martin probabilistic counting for POS_NOUN:\n",
      "\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 1: 8192 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 2: 2048 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 3: 512 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 4: 4096 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 5: 8192 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 6: 4096 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 7: 1024 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 8: 1024 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 9: 256 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 10: 2048 distinct words\n",
      "* Average of estimates: 3148.8\n",
      "* Median  of estimates: 2048.0\n",
      "\n",
      "Flajolet-Martin probabilistic counting for POS_ADJECTIVE:\n",
      "\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 1: 512 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 2: 65536 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 3: 256 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 4: 1024 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 5: 512 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 6: 1024 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 7: 8192 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 8: 1024 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 9: 256 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 10: 4096 distinct words\n",
      "* Average of estimates: 8243.2\n",
      "* Median  of estimates: 1024.0\n",
      "\n",
      "Flajolet-Martin probabilistic counting for POS_VERB:\n",
      "\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 1: 4096 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 2: 64 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 3: 128 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 4: 512 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 5: 1024 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 6: 1024 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 7: 4096 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 8: 256 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 9: 8192 distinct words\n",
      "- Read 1000/3000 words so far\n",
      "- Read 2000/3000 words so far\n",
      "- Read 3000/3000 words so far\n",
      "Estimate on pass 10: 1024 distinct words\n",
      "* Average of estimates: 2041.6\n",
      "* Median  of estimates: 1024.0\n"
     ]
    }
   ],
   "source": [
    "print('Flajolet-Martin probabilistic counting for POS_NOUN:\\n')\n",
    "flajolet_martin_alg_avg_med(10, POS_NOUN)\n",
    "print()\n",
    "print('Flajolet-Martin probabilistic counting for POS_ADJECTIVE:\\n')\n",
    "flajolet_martin_alg_avg_med(10, POS_ADJECTIVE)\n",
    "print()\n",
    "print('Flajolet-Martin probabilistic counting for POS_VERB:\\n')\n",
    "flajolet_martin_alg_avg_med(10, POS_VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"blue\">\n",
    "    To avoid that the computations of average and mean lasted more than 15 minutes, I decide to put a lowest threshold of words to read (max_words=3000). Above we can observe the different values of distinct words for each pass (total of 10 passes for each type of word belonging to (noun, adjective, verb).\n",
    "    \n",
    "   For POS_NOUN we obtain: \n",
    "    \n",
    "    * Average of estimates: 4249.6    \n",
    "    * Median  of estimates: 3072.0\n",
    "    \n",
    "   For POS_ADJECTIVE we obtain:\n",
    "\n",
    "    * Average of estimates: 2227.2\n",
    "    * Median  of estimates: 1024.0\n",
    "    \n",
    "   For POS_VERB we obtain: \n",
    "    \n",
    "    * Average of estimates: 1894.4    \n",
    "    * Median  of estimates: 512.0\n",
    "    \n",
    "\n",
    "   After analyzing the results, in my opinion the median seems more adequate for a probabilistic counting algorithm because its more robust than the average to extreme values and outliers. Therefore, the median is a better estimator for the algorithm\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
